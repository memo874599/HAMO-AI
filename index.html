<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ù…Ø³Ø§Ø¹Ø¯ Ù…Ø±ÙƒØ² Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø°ÙƒÙŠ HAMO</title>
    <style>
        /*
        ========================================
        --- NEW THEME: CRIMSON & AMETHYST (DARK & ELEGANT) ---
        ========================================
        */
        :root {
            /* Deep, Dark, and Rich Palette */
            --bg-gradient-start: #1a0a1a;  /* Deep Amethyst */
            --bg-gradient-end: #0c0c0d;    /* Near Black */
            --surface-color: rgba(26, 22, 34, 0.75); /* Dark Purple Frosted Glass */
            --primary-color: #b91c1c;      /* Deep Crimson Red */
            --secondary-color: #7e22ce;   /* Rich Royal Purple */
            --error-color: #ef4444;       /* Standard, clean red for errors */
            --text-color: #EAE6FF;        /* Pale Lavender White for readability */
            --text-muted-color: #8a7aa8;  /* Faded Starlight, good contrast */
            --border-color-primary: rgba(185, 28, 28, 0.4);  /* Crimson Red Glow Border */
            --border-color-secondary: rgba(126, 34, 206, 0.4); /* Purple Glow Border */
            --font-main: 'Tajawal', 'Segoe UI', 'Roboto', sans-serif;
        }

        @import url('https://fonts.googleapis.com/css2?family=Tajawal:wght@400;500;700&display=swap');
        
        @keyframes animated-gradient {
            0% { background-position: 0% 50%; }
            50% { background-position: 100% 50%; }
            100% { background-position: 0% 50%; }
        }

        @keyframes fade-in-up {
            from { opacity: 0; transform: translateY(30px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        body {
            font-family: var(--font-main);
            background: linear-gradient(135deg, var(--bg-gradient-start), var(--bg-gradient-end));
            background-size: 200% 200%;
            animation: animated-gradient 30s ease infinite;
            color: var(--text-color);
            margin: 0;
            padding: 2rem;
            font-size: 16px;
            min-height: 100vh;
            overflow-x: hidden;
        }

        .container {
            max-width: 1600px;
            margin: auto;
            display: grid;
            grid-template-columns: repeat(12, 1fr);
            gap: 2rem;
        }

        .grid-span-12 { grid-column: span 12; }
        .grid-span-6 { grid-column: span 6; }
        @media (max-width: 1200px) {
            .grid-span-6 { grid-column: span 12; }
        }

        .card {
            background-color: var(--surface-color);
            backdrop-filter: blur(8px);
            -webkit-backdrop-filter: blur(8px);
            border-radius: 12px;
            padding: 1.5rem;
            border: 1px solid var(--border-color-primary); /* Default border is Red */
            display: flex;
            flex-direction: column;
            box-shadow: 0 0 15px rgba(185, 28, 28, 0.15), 0 0 30px rgba(185, 28, 28, 0.1); /* Default glow is Red */
            transition: transform 0.4s cubic-bezier(0.25, 0.8, 0.25, 1), box-shadow 0.4s cubic-bezier(0.25, 0.8, 0.25, 1);
            position: relative;
            opacity: 0;
            animation: fade-in-up 0.6s ease-out forwards;
        }
        
        /* THEME OVERRIDE FOR PURPLE CARDS */
        .card-theme-secondary {
            border-color: var(--border-color-secondary);
            box-shadow: 0 0 15px rgba(126, 34, 206, 0.15), 0 0 30px rgba(126, 34, 206, 0.1);
        }

        /* Staggered animation for cards */
        .card:nth-of-type(1) { animation-delay: 0.1s; }
        .card:nth-of-type(2) { animation-delay: 0.2s; }
        .card:nth-of-type(3) { animation-delay: 0.3s; }
        .card:nth-of-type(4) { animation-delay: 0.4s; }
        .card:nth-of-type(5) { animation-delay: 0.5s; }

        .card:hover {
            transform: translateY(-5px) scale(1.02);
            box-shadow: 0 0 25px var(--primary-color), 0 0 40px rgba(185, 28, 28, 0.3);
        }
        .card-theme-secondary:hover {
            box-shadow: 0 0 25px var(--secondary-color), 0 0 40px rgba(126, 34, 206, 0.3);
        }

        .card-header {
            display: flex; justify-content: space-between; align-items: center;
            margin-bottom: 1rem; padding-bottom: 1rem;
            border-bottom: 1px solid var(--border-color-primary); /* Default is Red */
        }
        .card-theme-secondary .card-header {
            border-bottom-color: var(--border-color-secondary); /* Purple for themed cards */
        }

        .card-title {
            font-size: 1.4rem; font-weight: 700; margin: 0;
            color: var(--primary-color); /* Default title is Red */
            display: flex; align-items: center;
            text-shadow: 0 0 10px var(--primary-color), 0 0 20px rgba(185, 28, 28, 0.5);
        }
        .card-theme-secondary .card-title {
            color: var(--secondary-color); /* Purple title for themed cards */
            text-shadow: 0 0 10px var(--secondary-color), 0 0 20px rgba(126, 34, 206, 0.5);
        }

        .card-title .emoji { margin-left: 12px; font-size: 1.5em; }
        
        textarea, select, input {
            width: 100%; padding: 0.85rem;
            background-color: rgba(10, 5, 25, 0.8);
            border: 1px solid var(--border-color-primary);
            color: var(--text-color); border-radius: 8px;
            font-family: var(--font-main); font-size: 1rem;
            box-sizing: border-box; resize: vertical;
            box-shadow: inset 0 0 10px rgba(185, 28, 28, 0.2);
            transition: border-color 0.3s ease, box-shadow 0.3s ease;
        }

        textarea:focus, select:focus, input:focus {
            outline: none;
            border-color: var(--primary-color);
            box-shadow: inset 0 0 10px rgba(185, 28, 28, 0.4), 0 0 15px var(--primary-color);
        }

        .transcript-box, .reply-box {
            height: 200px; overflow-y: auto; background-color: rgba(0,0,0,0.5);
            padding: 1rem; border-radius: 8px; line-height: 1.7;
            white-space: pre-wrap; word-wrap: break-word;
            box-shadow: inset 0 2px 10px rgba(0,0,0,0.4);
            border: 1px solid var(--border-color-primary);
        }
        
        /* Diff colors: additions are purple, deletions are red */
        .diff-container ins { background-color: rgba(126, 34, 206, 0.2); color: var(--secondary-color); text-decoration: none; }
        .diff-container del { background-color: rgba(185, 28, 28, 0.2); color: var(--primary-color); text-decoration: none; }

        button {
            padding: 0.85rem 1.7rem; border-radius: 8px;
            border: 1px solid transparent; font-family: var(--font-main);
            font-weight: 700; cursor: pointer; transition: all 0.3s ease-in-out;
            font-size: 1rem; box-shadow: 0 4px 15px rgba(0, 0, 0, 0.5);
            display: inline-flex; align-items: center; justify-content: center;
            gap: 8px; color: var(--text-color); text-transform: uppercase;
            letter-spacing: 1px;
        }

        .btn-primary { background-color: var(--primary-color); border-color: var(--primary-color); color: #fff; text-shadow: 0 0 5px #000;}
        .btn-primary:hover:not(:disabled) {
             background-color: #d93131;
             box-shadow: 0 0 20px var(--primary-color), 0 0 30px rgba(185, 28, 28, 0.4);
             transform: translateY(-2px);
        }
        .btn-secondary { background-color: var(--secondary-color); border-color: var(--secondary-color); color: #fff; text-shadow: 0 0 5px #000;}
        .btn-secondary:hover:not(:disabled) {
            background-color: #9b48f2;
            box-shadow: 0 0 20px var(--secondary-color), 0 0 30px rgba(126, 34, 206, 0.4);
            transform: translateY(-2px);
        }
        button:disabled { background-color: #333; cursor: not-allowed; opacity: 0.5; box-shadow: none; transform: none;}

        .controls { display: flex; gap: 1rem; align-items: center; flex-wrap: wrap;}
        
        .status-bar {
            text-align: center; padding: 0.75rem; border-radius: 8px; font-weight: 500;
            transition: all 0.5s ease; position: fixed; bottom: 1rem; left: 50%;
            transform: translateX(-50%); width: 400px; max-width: 95%;
            z-index: 1000; box-shadow: 0 0 20px rgba(0,0,0,0.5);
            background-color: var(--surface-color); backdrop-filter: blur(5px);
            border: 1px solid var(--border-color-secondary); color: var(--text-color);
            text-shadow: 0 0 5px rgba(0,0,0,0.5);
        }
        .status-processing {
             border-color: #ffc107; color: #ffc107;
             box-shadow: 0 0 15px #ffc107, 0 0 30px rgba(255, 193, 7, 0.4);
        }

        .analysis-pills { display: flex; flex-wrap: wrap; gap: 0.75rem; }
        .pill { 
            padding: 0.5rem 1rem; border-radius: 20px; font-size: 0.9rem; 
            background: linear-gradient(135deg, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0.05));
            border: 1px solid rgba(255,255,255,0.1);
            color: var(--text-color); backdrop-filter: blur(2px);
        }
        .pill strong { font-weight: 700; color: var(--primary-color); text-shadow: 0 0 8px var(--primary-color); }
        
        .info-panel {
            background-color: rgba(185, 28, 28, 0.1);
            border: 1px solid var(--primary-color);
            padding: 1rem; border-radius: 8px; font-size: 0.95rem;
            line-height: 1.6; color: var(--text-color);
            backdrop-filter: blur(3px);
        }
        
        @media (max-width: 600px) {
            body { padding: 1rem; }
            .container { gap: 1rem; }
            .card-title { font-size: 1.2rem; }
        }

    </style>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/diff/5.1.0/diff.min.js"></script>
</head>
<body>
    <div class="container">
        <!-- Header Controls (Default Red Theme) -->
        <div class="card grid-span-12">
            <div class="card-header">
                <!-- No title needed here -->
            </div>
            <div class="controls">
                <button id="startButton" class="btn-primary">
                    <span class="emoji">â–¶ï¸</span>
                    Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹
                </button>
                <button id="stopButton" class="btn-danger" disabled>
                    <span class="emoji">â¹ï¸</span>
                    Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹
                </button>
            </div>
        </div>

        <!-- Customer Voice Input (Default Red Theme) -->
        <div class="card grid-span-6">
            <div class="card-header">
                <h2 class="card-title">
                     Ø§Ù„Ù…Ø¯Ø®Ù„ Ø§Ù„ØµÙˆØªÙŠ Ù„Ù„Ø¹Ù…ÙŠÙ„
                    <span class="emoji">ğŸ™ï¸</span>
                </h2>
            </div>
            <div class="info-panel" style="margin-bottom: 1rem;">
                <strong>Ø¥Ø±Ø´Ø§Ø¯Ø§Øª Ø§Ù„ØµÙˆØª:</strong> Ù„Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ ØµÙˆØª Ø§Ù„Ø¹Ù…ÙŠÙ„ ÙˆØµÙˆØªÙƒ Ù…Ø¹Ø§Ù‹ØŒ ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯ ØªÙˆØ¬ÙŠÙ‡ Ø§Ù„ØµÙˆØª ÙÙŠ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ´ØºÙŠÙ„ (Windows). Ø§ØªØ¨Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ù„Ø¥Ø¹Ø¯Ø§Ø¯ "Listen to this device" Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†Ùƒ ÙˆØªÙˆØ¬ÙŠÙ‡Ù‡ Ø¥Ù„Ù‰ VB-Audio Cable.
            </div>
            <div class="transcript-box" id="transcriptContainer">
                <span id="finalTranscript"></span>
                <span id="interimTranscript"></span>
            </div>
            <textarea id="customerSpeech" rows="4" style="margin-top: 1rem;" readonly placeholder="ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬ Ø³ÙŠØ¸Ù‡Ø± Ù‡Ù†Ø§..."></textarea>
        </div>

         <!-- AI Suggested Reply (Purple Theme) -->
        <div class="card grid-span-6 card-theme-secondary">
             <div class="card-header">
                <h2 class="card-title">
                    Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ
                    <span class="emoji">âœ¨</span> <span class="emoji">ğŸ’¬</span>
                </h2>
            </div>
            <textarea id="suggestedReply" rows="6" placeholder="Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ø³ÙŠØ¸Ù‡Ø± Ù‡Ù†Ø§..."></textarea>
        </div>


        <!-- AI Analysis (Default Red Theme) -->
        <div class="card grid-span-6">
            <div class="card-header">
                <h2 class="card-title">
                    ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ
                    <span class="emoji">ğŸ”¬</span> <span class="emoji">ğŸ§ </span>
                </h2>
            </div>
            <textarea id="customerSpeechTranslated" rows="3" readonly placeholder="Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©..."></textarea>
            <div class="analysis-pills" style="margin-top: 1rem;">
                <div class="pill">Ø§Ù„Ù…Ø´Ø§Ø¹Ø±: <strong id="sentiment">...</strong></div>
                <div class="pill">Ù†ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ„: <strong id="intent">...</strong></div>
            </div>
        </div>

        <!-- Agent Reply & Diff (Purple Theme) -->
        <div class="card grid-span-6 card-theme-secondary">
             <div class="card-header">
                <h2 class="card-title">
                    Ø±Ø¯ Ø§Ù„Ù…ÙˆØ¸Ù ÙˆÙ…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª
                     <span class="emoji">ğŸ“</span> <span class="emoji">â†”ï¸</span>
                </h2>
            </div>
             <textarea id="agentReply" rows="6" placeholder="Ø§ÙƒØªØ¨ Ø±Ø¯Ùƒ Ù‡Ù†Ø§ Ø£Ùˆ Ø§Ù†Ø³Ø® Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­..."></textarea>
            <div class="controls" style="margin-top: 1rem;">
                 <button id="copyReply" class="btn-secondary">
                     <span class="emoji">ğŸ“‹</span>
                     Ù†Ø³Ø® Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­
                 </button>
                 <button id="sendReplyButton" class="btn-primary">
                     <span class="emoji">ğŸš€</span>
                     Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø¯
                 </button>
                 <button id="useCanned" class="btn-secondary">
                      <span class="emoji">ğŸ“š</span>
                     Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø¯ Ø¬Ø§Ù‡Ø²
                 </button>
                 <select id="cannedResponsesSelect" style="flex-grow: 1;">
                     <!-- Options will be populated by JS -->
                 </select>
            </div>
            <strong style="margin-top: 1rem; display: block;">Ù…Ù‚Ø§Ø±Ù†Ø© Ø±Ø¯Ùƒ Ø¨Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­:</strong>
            <div id="diffOutput" class="diff-container reply-box" style="height: 150px;"></div>
            <strong style="margin-top: 1rem; display: block;">Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ù…Ù„ (Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ÙˆØ¯ Ø£ÙØ¶Ù„):</strong>
            <textarea id="jobContext" rows="4" placeholder="Ù…Ø«Ø§Ù„: Ø£Ù†Ø§ ÙˆÙƒÙŠÙ„ Ø¯Ø¹Ù… ÙÙ†ÙŠ ÙÙŠ Ø´Ø±ÙƒØ© Ø§ØªØµØ§Ù„Ø§Øª. Ù…Ù‡Ù…ØªÙŠ Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ÙÙˆØ§ØªÙŠØ± ÙˆØªÙ‚Ø¯ÙŠÙ… Ø®ØµÙˆÙ…Ø§Øª. ÙƒÙ† ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙˆÙ…Ø­ØªØ±ÙØ§Ù‹."></textarea>
        </div>

    </div>

    <div id="statusBar" class="status-bar status-idle">Ø¬Ø§Ù‡Ø²</div>

<script>
    // The JavaScript remains unchanged as the core logic is sound.
    // All visual changes have been handled purely via CSS.
    // DOM Elements
    const startButton = document.getElementById('startButton');
    const stopButton = document.getElementById('stopButton');
    // Removed useOpenAIToggle
    const finalTranscriptElem = document.getElementById('finalTranscript');
    const interimTranscriptElem = document.getElementById('interimTranscript');
    const customerSpeechElem = document.getElementById('customerSpeech');
    const customerSpeechTranslatedElem = document.getElementById('customerSpeechTranslated');
    const sentimentElem = document.getElementById('sentiment');
    const intentElem = document.getElementById('intent');
    const suggestedReplyElem = document.getElementById('suggestedReply');
    const agentReplyElem = document.getElementById('agentReply');
    const copyReplyButton = document.getElementById('copyReply');
    const useCannedButton = document.getElementById('useCanned');
    const cannedResponsesSelect = document.getElementById('cannedResponsesSelect');
    const diffOutputElem = document.getElementById('diffOutput');
    const jobContextElem = document.getElementById('jobContext');
    const statusBar = document.getElementById('statusBar');
    const sendReplyButton = document.getElementById('sendReplyButton'); // Get the Send Reply button

    // --- API KEYS & CONFIG ---
    // WARNING: These keys are visible in client-side code. For personal use only.
    // DO NOT deploy this to a public server with real keys.
    // Removed OpenAI API Key and Assistant ID
    const GEMINI_API_KEY = "AIzaSyBNMPsHNr6f6Gn1p_tXJvHrogy_BjWVcIc";

    // MODIFIED: Reduced pause delay for faster AI triggering after speech stops
    const SPEECH_PAUSE_DELAY = 1500; // ms (reduced from 2500ms)

    // --- Microphone and Speech Recognition Variables ---
    let recognition; // Use 'let' so we can reassign or nullify
    let isListening = false;
    let isProcessingAI = false; // Flag to indicate if AI processing is active
    let finalTranscript = '';
    let interimTranscript = '';
    let speechTimeout; // Timer for detecting pauses in speech
    let isManuallyStopped = false; // Flag to track if stop button was clicked

    // Ensure SpeechRecognition API is available
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
        updateStatus('Ù…ØªØµÙØ­Ùƒ Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…. Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬ÙˆØ¬Ù„ ÙƒØ±ÙˆÙ….', 'error');
        startButton.disabled = true;
    }

    // Function to handle pauses in speech and trigger AI processing
    const handleSpeechPause = () => {
        const transcriptToProcess = finalTranscript.trim();
        // Only process if there's actual speech and no AI processing is already happening
        if (transcriptToProcess.length > 0 && !isProcessingAI) {
            console.log("Speech pause detected. Processing final transcript segment:", transcriptToProcess);
            processFinalTranscriptSegment(transcriptToProcess);
            finalTranscript = ''; // Clear final transcript after sending for processing
            interimTranscriptElem.textContent = ''; // Clear interim display
            interimTranscript = ''; // Clear interim variable
        }
    };
    // --- End Microphone and Speech Recognition Variables & handleSpeechPause ---


    // --- Core Functions ---

    function updateStatus(message, type = 'idle', state = 'idle') {
        statusBar.textContent = message;
        statusBar.className = 'status-bar'; // Reset
        statusBar.classList.add(`status-${type}`);

        // Only update isProcessingAI state if state param is explicitly provided
        if (state === 'processing') {
             isProcessingAI = true;
        } else if (state === 'idle') {
             isProcessingAI = false;
        }
        // If state is not explicitly 'processing' or 'idle', keep current isProcessingAI state


        // Manage button disabled state based on current isListening and isProcessingAI flags
        startButton.disabled = isListening || isProcessingAI;
        stopButton.disabled = !isListening || isProcessingAI;
    }

    // --- MODIFIED callLLM Function (Gemini Exclusive, always Flash) ---
    async function callLLM(task, text, context = '', modelOverride = null) { // Removed isGemini param
        let payload;
        let apiUrl;
        let headers = { 'Content-Type': 'application/json' };

        const jobContext = jobContextElem.value || 'General customer service. Be polite and professional.';
        let fullPrompt = `Job Context: ${jobContext}\n\n`;

        // MODIFIED: Always use gemini-2.0-flash unless overridden
        const selectedGeminiModel = modelOverride || 'gemini-2.0-flash';


        switch (task) {
            case 'extract':
                fullPrompt += `Given the following conversation transcript, identify and extract only the text spoken by the customer. Ignore any parts that seem to be the agent's speech or background noise. If no clear customer speech is present, return the original text. Provide ONLY the customer's speech, no other text or formatting.\n\nTranscript: "${text}"`;
                // Model already defaults to gemini-2.0-flash or uses override
                break;
            case 'translate':
                // Refined Translation Prompt
                fullPrompt += `Translate the following English text to Arabic. Provide only the Arabic translation:\n\nEnglish: "${text}"\nArabic:`;
                // Model already defaults to gemini-2.0-flash or uses override
                break;
            case 'sentiment':
                fullPrompt += `Analyze the sentiment of the following text. Respond with only one word: "Positive", "Negative", or "Neutral".\n\nText: "${text}"\nSentiment:`;
                 // Model already defaults to gemini-2.0-flash or uses override
                break;
            case 'intent':
                fullPrompt += `Classify the intent of the following customer query into one of these categories: "General Inquiry", "Technical Support", "Billing Issue", "Product Information", "Order Status", "Complaint", "Feedback", "Other". Focus solely on the category.\n\nQuery: "${text}"\nIntent:`;
                 // Model already defaults to gemini-2.0-flash or uses override
                break;
            case 'suggest':
                fullPrompt += `You are an expert call center agent. Based on the customer's speech and the work context provided, provide the best, most professional, and concise reply in English. Ensure the reply is grammatically correct and polished. Provide only the reply, do not include any conversational filler like "Here is a suggestion:".\n\nCustomer Input: "${text}"\nPolite Reply:`;
                 // Model already defaults to gemini-2.0-flash or uses override
                break;
            default:
                console.error("Unknown AI task:", task);
                return null;
        }

        if (!GEMINI_API_KEY || GEMINI_API_KEY === "") {
            console.error('Gemini API key is missing or invalid.');
            throw new Error('Gemini API key is not configured.');
        }
        apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/${selectedGeminiModel}:generateContent?key=${GEMINI_API_KEY}`;
        payload = { contents: [{ role: "user", parts: [{ text: fullPrompt }] }] };


        try {
            const response = await fetch(apiUrl, {
                method: 'POST',
                headers: headers,
                body: JSON.stringify(payload)
            });

            if (!response.ok) {
                const errorData = await response.json();
                console.error(`API Error Response for task "${task}":`, errorData); // Enhanced error log
                throw new Error(`API error (${response.status}): ${errorData.error?.message || response.statusText}`);
            }

            const data = await response.json();
            console.log(`Raw API response for task "${task}" (Gemini):`, data); // Crucial log

            let responseText = null;
             // Corrected response parsing for Gemini structure
            if (data.candidates && data.candidates.length > 0 && data.candidates[0].content && data.candidates[0].content.parts && data.candidates[0].content.parts.length > 0) {
                responseText = data.candidates[0].content.parts[0].text;
            } else {
                console.error("Gemini response structure unexpected or empty for task", task, ":", data); // Added specific error log
                // Provide more info if generation feedback is available
                if (data.promptFeedback && data.promptFeedback.blockReason) {
                    console.error("Gemini prompt feedback block reason:", data.promptFeedback.blockReason);
                    // You might want to surface this to the user
                }
                if (data.candidates && data.candidates.length > 0 && data.candidates[0].finishReason) {
                     console.error("Gemini candidate finish reason:", data.candidates[0].finishReason);
                }
                throw new Error('Gemini returned no valid response or unexpected structure.');
            }

            if (responseText) {
                console.log(`Successfully extracted response text for task "${task}":`, responseText.trim()); // Log successful extraction
                return responseText.trim();
            } else {
                 // This case should be covered by the checks above, but as a safeguard:
                console.error(`Failed to extract text from Gemini response for task "${task}". ResponseText is null or empty. Raw data:`, data); // Log why extraction failed
                throw new Error(`Failed to extract text from Gemini response.`);
            }

        } catch (error) {
            console.error(`Error in callLLM for task "${task}" with Gemini:`, error);
            throw error; // Re-throw to be caught by processFinalTranscriptSegment
        }
    }
    // --- END MODIFIED callLLM Function ---

    // Ensure `processFinalTranscriptSegment` is defined correctly BEFORE `startListening` calls it
    async function processFinalTranscriptSegment(transcript) {
        if (isProcessingAI) {
            console.warn("AI processing already in progress. Skipping new segment.");
            return;
        }
        updateStatus('Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¨Ø§Ù„Ù†Øµ...', 'processing', 'processing'); // Set status to processing

        try {
            // Use callLLM with Gemini Flash for extraction
            const extractedSpeech = await callLLM('extract', transcript, null, 'gemini-2.0-flash').catch(e => {
                console.error("Error during extraction:", e);
                return null; // Return null on error
            });
            customerSpeechElem.value = extractedSpeech || "Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ù„Ø§Øµ ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ÙˆØ¶ÙˆØ­.";

            // If customer speech is extracted, proceed with other AI tasks in parallel
            if (extractedSpeech && extractedSpeech.trim() !== '' && extractedSpeech !== "Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ù„Ø§Øµ ÙƒÙ„Ø§Ù… Ø§Ù„Ø¹Ù…ÙŠÙ„ Ø¨ÙˆØ¶ÙˆØ­.") {

                // Run translation, sentiment, intent, and suggestion in parallel using gemini-2.0-flash
                const [translated, sentiment, intent, suggestion] = await Promise.all([
                    callLLM('translate', extractedSpeech, null, 'gemini-2.0-flash').catch(e => { // MODIFIED: Use gemini-2.0-flash
                        console.error("Translation API Error:", e); // Specific error log for translation
                        return null;
                    }),
                    callLLM('sentiment', extractedSpeech, null, 'gemini-2.0-flash').catch(e => {
                        console.error("Sentiment API Error:", e); // Specific error log for sentiment
                        return null;
                    }),
                    callLLM('intent', extractedSpeech, null, 'gemini-2.0-flash').catch(e => {
                        console.error("Intent API Error:", e); // Specific error log for intent
                        return null;
                    }),
                    callLLM('suggest', extractedSpeech, jobContextElem.value, 'gemini-2.0-flash').catch(e => { // MODIFIED: Ensure gemini-2.0-flash
                        console.error("Suggestion API Error:", e); // Specific error log for suggestion
                        return null;
                    })
                ]);

                customerSpeechTranslatedElem.value = translated || 'ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ±Ø¬Ù…Ø©.'; // Update textarea value
                sentimentElem.textContent = sentiment || 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯';
                intentElem.textContent = intent || 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯';
                suggestedReplyElem.value = suggestion || 'Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯.'; // This should now populate

                const sentimentColors = {
                    "Positive": "#10b981", "Negative": "#ef4444", "Neutral": "#3b82f6"
                };
                sentimentElem.style.color = sentimentColors[sentiment] || 'var(--text-color)';

                updateStatus('Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„', 'success');
            } else {
                // If extraction failed or was empty
                updateStatus('ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„: Ù„Ù… ÙŠØªÙ… Ø§ÙƒØªØ´Ø§Ù ÙƒÙ„Ø§Ù… Ù„Ù„Ø¹Ù…ÙŠÙ„.', 'info', 'idle');
                customerSpeechTranslatedElem.value = "Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø³ØªØ¸Ù‡Ø± Ù‡Ù†Ø§...";
                sentimentElem.textContent = "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„...";
                intentElem.textContent = "Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„...";
                suggestedReplyElem.value = "Ø³ÙŠØ¸Ù‡Ø± Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ø³ÙŠØ¸Ù‡Ø± Ù‡Ù†Ø§...";
                sentimentElem.style.color = 'var(--text-color)';
            }
        } catch (e) {
            console.error("Error processing final transcript segment:", e);
            updateStatus(`Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: ${e.message}`, 'error');
        } finally {
            // isProcessingAI flag is managed by updateStatus calls with state param
            // Reset status after a delay if not actively listening AND not processing AI
            setTimeout(() => { if (!isListening && !isProcessingAI) updateStatus('Ø¬Ø§Ù‡Ø²', 'idle'); }, 3000);
        }
    }


    // --- REWRITTEN startListening Function (Enhanced Permission/Stability) ---
    const startListening = async () => {
        if (isListening) {
            console.log("Already listening. Ignoring start request.");
            return; // Prevent starting if already active
        }

        // Reset the manual stop flag
        isManuallyStopped = false;

        // Clear previous data and UI elements for a fresh start
        finalTranscript = ''; // Clear final transcript variable
        interimTranscript = ''; // Clear interim transcript variable
        finalTranscriptElem.textContent = ''; // Clear final transcript display
        interimTranscriptElem.textContent = ''; // Clear interim transcript display
        customerSpeechElem.value = ''; // Clear customer speech textarea
        customerSpeechTranslatedElem.value = ''; // Clear translated speech textarea
        sentimentElem.textContent = '...'; // Reset sentiment display
        intentElem.textContent = '...'; // Reset intent display
        suggestedReplyElem.value = 'Ø³ÙŠØ¸Ù‡Ø± Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ø³ÙŠØ¸Ù‡Ø± Ù‡Ù†Ø§...'; // Reset to original placeholder
        agentReplyElem.value = ''; // Clear agent reply textarea
        displayDiff(); // Update diff display
        sentimentElem.style.color = 'var(--text-color)'; // Reset sentiment color

        updateStatus('Ø¬Ø§Ø±ÙŠ Ø·Ù„Ø¨ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†...', 'info'); // Indicate permission request
        startButton.disabled = true; // Disable start button immediately
        stopButton.disabled = true; // Disable stop button until listening starts

        try {
            let permissionState = 'prompt'; // Assume prompt initially

            // --- Check current microphone permission state ---
            // Ensure navigator.permissions and query method are available
            if (navigator.permissions && typeof navigator.permissions.query === 'function') {
                try {
                    const permissionStatus = await navigator.permissions.query({ name: "microphone" });
                    permissionState = permissionStatus.state;
                    console.log("Microphone permission state from query:", permissionState);

                    // If permission is already granted, we don't need to call getUserMedia again.
                    // Proceed directly to starting recognition if it's granted.
                    if (permissionState === 'granted') {
                        console.log("Microphone permission already granted. Proceeding to start recognition.");
                         // Initialize recognition instance if not already.
                         if (!recognition) {
                              initializeRecognition(); // Call a helper function to set up recognition instance and handlers
                         }
                         // Attempt to start recognition
                         try {
                              recognition.start();
                             // onstart handler will update status and buttons
                         } catch (startErr) {
                             console.error("Error calling recognition.start() after permission granted:", startErr);
                             handleRecognitionStartError(startErr); // Handle start error
                         }
                         return; // Exit startListening after attempting to start recognition
                    }
                } catch (queryErr) {
                     console.warn("navigator.permissions.query failed, falling back to getUserMedia:", queryErr);
                     // Fallback: If query fails (e.g., browser doesn't support query for 'microphone'),
                     // proceed to getUserMedia which will prompt if needed.
                }
            } else {
                 console.warn("navigator.permissions API or query method not available. Will rely on getUserMedia to prompt for permission.");
            }
            // --- End permission check ---


            // If permission is not granted ('prompt' or 'denied' from query, or query failed/not supported), call getUserMedia to prompt or fail
            console.log(`Microphone permission state is '${permissionState}' or query failed. Calling getUserMedia...`);
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            // Immediately stop the tracks after confirming access, as SpeechRecognition manages its own stream.
            stream.getTracks().forEach(track => track.stop());
            console.log("Microphone access obtained via getUserMedia and stream stopped.");


            // Initialize SpeechRecognition instance if not already (should be null if we reach here without permission granted)
            if (!recognition) {
                initializeRecognition(); // Call helper function
            }

             // Attempt to start recognition
             try {
                 recognition.start();
                 // onstart handler will update status and buttons
            } catch (startErr) {
                 console.error("Error calling recognition.start() after getUserMedia:", startErr);
                 handleRecognitionStartError(startErr); // Handle start error
            }


        } catch (err) {
            // Handle errors from getUserMedia or permission check (microphone access denial/failure)
            console.error("Failed during microphone permission check or getUserMedia:", err);
            clearTimeout(speechTimeout); // Clear any pending timeout
            isListening = false; // Ensure listening state is false
            isProcessingAI = false; // Ensure AI processing state is false
             recognition = null; // Clear recognition instance if mic access failed

            let errorMessage = 'ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹. ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØµÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙˆØ¥Ø°Ù† Ø§Ù„ÙˆØµÙˆÙ„.';
            if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
                errorMessage = 'ØªÙ… Ø±ÙØ¶ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªØµÙØ­.';
            } else if (err.name === 'NotFoundError') {
                errorMessage = 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ÙŠØ±Ø¬Ù‰ ØªÙˆØµÙŠÙ„ Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.';
            } else if (err.name === 'TypeError') {
                 errorMessage = 'Ø®Ø·Ø£ ÙÙŠ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù‡Ø§Ø². Ù‚Ø¯ ØªÙƒÙˆÙ† Ù‡Ù†Ø§Ùƒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØµÙˆØª.';
            } else if (err.name === 'AbortError') {
                 errorMessage = 'ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø·Ù„Ø¨ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.';
            }


            updateStatus(errorMessage, 'error');
            startButton.disabled = false;
            stopButton.disabled = true;
        }
    };
    // --- END REWRITTEN startListening Function ---

    // --- Helper function to initialize SpeechRecognition instance and handlers ---
    const initializeRecognition = () => {
         console.log("Initializing new SpeechRecognition instance.");
         recognition = new SpeechRecognition();
         recognition.lang = 'en-US'; // Set language to English for recognition
         recognition.continuous = true; // Enable continuous recognition
         recognition.interimResults = true; // Show interim results

         recognition.onstart = () => {
             isListening = true;
             // Status update and button states are managed by startListening's success path
             updateStatus('ÙŠØ³ØªÙ…Ø¹ Ø§Ù„Ø¢Ù†...', 'listening');
             startButton.disabled = true;
             stopButton.disabled = false;
             console.log("Speech recognition started.");
         };

         recognition.onresult = (event) => {
             clearTimeout(speechTimeout); // Reset silence timer

             let interimResult = '';
             let finalResult = '';

             // Process all results from this event
             for (let i = event.resultIndex; i < event.results.length; ++i) {
                 const transcriptPart = event.results[i][0].transcript;
                 if (event.results[i].isFinal) {
                     finalResult += transcriptPart + ' ';
                 } else {
                     interimResult += transcriptPart;
                 }
             }

             finalTranscript += finalResult; // Accumulate final transcript
             interimTranscript = interimResult; // Update interim transcript

             finalTranscriptElem.textContent = finalTranscript;
             interimTranscriptElem.textContent = interimTranscript;

             // Trigger processing timeout if new final text or significant interim exists
             if (finalResult.trim().length > 0 || interimTranscript.trim().length > 15) {
                  speechTimeout = setTimeout(handleSpeechPause, SPEECH_PAUSE_DELAY);
             }
         };

         recognition.onend = () => {
             console.log("Speech recognition ended. isListening:", isListening, "isManuallyStopped:", isManuallyStopped);
             clearTimeout(speechTimeout); // Clear any pending timeout

             // Process any remaining final transcript segment
             const transcriptToProcess = finalTranscript.trim();
             if (transcriptToProcess.length > 0 && !isProcessingAI) {
                 processFinalTranscriptSegment(transcriptToProcess); // This handles clearing finalTranscript
                 // interim transcript is cleared in handleSpeechPause or stopListening
             } else {
                 // If no transcript was processed, just ensure interim display is clear
                 interimTranscriptElem.textContent = '';
                 interimTranscript = '';
             }

             // CRITICAL: If 'isListening' is still true AND it was NOT manually stopped, attempt restart.
             // isListening should be true ONLY if onend was triggered unexpectedly while we intended to be listening.
             if (isListening && !isManuallyStopped) {
                 console.log("Recognition ended unexpectedly, attempting restart...");
                 updateStatus('Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹...', 'info');
                 // Add a small delay before attempting restart to prevent immediate errors
                 setTimeout(() => {
                     // Double check flags before restarting - state might have changed during timeout
                     // Check recognition is not null - could be nullified by a fatal error that fired before onend
                     if (isListening && !isManuallyStopped && recognition) {
                         try {
                             recognition.start();
                             console.log("Recognition restart attempted.");
                         } catch (restartErr) {
                             console.error("Failed to restart Speech Recognition:", restartErr);
                             updateStatus('ÙØ´Ù„ ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹.', 'error');
                             isListening = false; // Ensure state is false if restart fails
                             startButton.disabled = false;
                             stopButton.disabled = true;
                             recognition = null; // Important: nullify if restart fails to allow fresh start
                         }
                     } else {
                         // If isListening became false or isManuallyStopped became true during timeout, finalize state.
                         console.log("Restart aborted: isListening is false, isManuallyStopped is true, or recognition is null.");
                         if (!isProcessingAI) updateStatus('ØªÙ… Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù', 'idle'); // Ensure final status if processing is done
                         startButton.disabled = false;
                         stopButton.disabled = true;
                     }
                 }, 500); // 500ms delay
             } else {
                 // If stopped intentionally (`isManuallyStopped` is true, `isListening` is false)
                 console.log("Recognition ended due to manual stop or previous error.");
                 recognition = null; // Nullify recognition for a clean state on next manual start
                 // Status and button states are handled by stopListening after it calls recognition.stop()
                 // Or by the error handler if it was a fatal error.
             }
         };

         recognition.onerror = (event) => {
             console.error("Speech Recognition Error:", event.error);
             clearTimeout(speechTimeout); // Clear timeout on error

             let errorMessage = `Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹: ${event.error}`;
             let statusType = 'error';
             let shouldNullifyRecognition = false; // Flag to indicate if recognition object is broken
             let shouldManuallyResume = false; // Flag for errors requiring user click to resume

             // Fatal errors that require user intervention or a fresh start
             const fatalErrors = ['not-allowed', 'audio-capture', 'network', 'service-not-allowed', 'language-not-supported'];

             if (fatalErrors.includes(event.error)) {
                 if (event.error === 'not-allowed') errorMessage = 'Ø®Ø·Ø£: ØªÙ… Ø±ÙØ¶ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªØµÙØ­.';
                 else if (event.error === 'audio-capture') errorMessage = 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ØµÙˆØª. ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆØµÙŠÙ„ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ÙˆØ§Ø®ØªÙŠØ§Ø±Ù‡ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.';
                 else if (event.error === 'network') errorMessage = 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§ØªØµØ§Ù„Ùƒ Ø¨Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª.';
                 else if (event.error === 'service-not-allowed') errorMessage = 'Ø§Ù„Ø®Ø¯Ù…Ø© ØºÙŠØ± Ù…Ø³Ù…ÙˆØ­ Ø¨Ù‡Ø§. Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø­Ø¸ÙˆØ±Ø© Ù…Ù† Ù‚Ø¨Ù„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø£Ùˆ Ø§Ù„Ø´Ø¨ÙƒØ©.';
                 else if (event.error === 'language-not-supported') errorMessage = 'Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©.';

                 console.error(`Fatal Speech Recognition Error: ${event.error}. Stopping recognition.`);
                 shouldNullifyRecognition = true; // Recognition object is likely unusable
                 shouldManuallyResume = true; // User must click start

             } else if (event.error === 'no-speech') {
                 errorMessage = 'Ù„Ù… ÙŠØªÙ… Ø§Ù„ÙƒØ´Ù Ø¹Ù† ÙƒÙ„Ø§Ù…. Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…ØªÙˆÙ‚Ù Ù…Ø¤Ù‚ØªÙ‹Ø§. Ø§Ù†Ù‚Ø± "Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹" Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù.'; // Inform user how to restart
                 statusType = 'info';
                 shouldManuallyResume = true; // User must click start
                 console.log("No speech detected. Treating as temporary pause requiring manual resume.");
                 // Do NOT nullify recognition here, it's still potentially usable.
                 // Do NOT automatically restart. Let onend handle state transition.

             } else if (event.error === 'aborted') {
                 errorMessage = 'ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…. Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…ØªÙˆÙ‚Ù Ù…Ø¤Ù‚ØªÙ‹Ø§. Ø§Ù†Ù‚Ø± "Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹" Ù„Ù„Ø§Ø³ØªØ¦Ù†Ø§Ù.'; // Inform user how to restart
                 statusType = 'info';
                 shouldManuallyResume = true; // User must click start
                 console.log("Recognition aborted. Treating as temporary pause requiring manual resume.");
                 // Do NOT nullify recognition here, it's still potentially usable.
                 // Do NOT automatically restart. Let onend handle state transition.

             } else if (event.error === 'bad-grammar') {
                 errorMessage = 'Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ (Bad Grammar).';
                 statusType = 'info';
                 console.log("Bad grammar error.");
                 // Not fatal, doesn't require manual restart, onend will handle potential restart if continuous
                 updateStatus(errorMessage, statusType); // Just update status and exit
                 return;
             }

             // For errors that require manual resume ('no-speech', 'aborted') or are fatal:
             if (shouldManuallyResume) {
                  // Ensure listening state is false
                  isListening = false;
                  // Update status
                  updateStatus(errorMessage, statusType, 'idle'); // Set state to idle
                  // Ensure buttons are correctly enabled/disabled
                  startButton.disabled = false;
                  stopButton.disabled = true;

                  // Process any pending transcript before cleaning up recognition
                  const transcriptToProcess = finalTranscript.trim();
                  if (transcriptToProcess.length > 0 && !isProcessingAI) {
                      processFinalTranscriptSegment(transcriptToProcess); // This handles clearing finalTranscript
                       // interim transcript is cleared in handleSpeechPause or below
                  }
                  // Clear interim display immediately
                  interimTranscriptElem.textContent = '';
                  interimTranscript = '';

                  // Nullify recognition object for fatal errors or if needed for clean restart
                  if (shouldNullifyRecognition) {
                      if (recognition) recognition.stop(); // Ensure stop is called before nullifying on fatal errors
                      recognition = null;
                  }
                  // Note: For 'no-speech'/'aborted', recognition is NOT nullified here,
                  // allowing recognition.start() to be called directly on the existing object by the user.
             }
             // For non-manual errors (like bad-grammar), onend will fire, potentially triggering a restart if !isManuallyStopped
         };
     };
     // --- End Initialize Recognition Helper ---

     // --- Helper function to handle errors when calling recognition.start() ---
     const handleRecognitionStartError = (startErr) => {
         console.error("Error calling recognition.start():", startErr);
         clearTimeout(speechTimeout); // Clear timeout on start failure
         isListening = false; // Ensure state is false

         let errorMessage = 'ÙØ´Ù„ Ø¨Ø¯Ø¡ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ†.';
         // Check if the error indicates it was already started (common if button clicked rapidly)
         if (startErr.message.includes("already started")) {
              errorMessage = 'Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù†Ø´Ø· Ø¨Ø§Ù„ÙØ¹Ù„.';
              isListening = true; // Correct the state if it was already started
              // Update status and button states to reflect that it IS listening
              updateStatus('ÙŠØ³ØªÙ…Ø¹ Ø§Ù„Ø¢Ù†...', 'listening');
              startButton.disabled = true;
              stopButton.disabled = false;
              return; // Exit if it was just already started
         }

         // For other start errors
         updateStatus(errorMessage, 'error');
         // Re-enable buttons if start failed and not already listening
         startButton.disabled = false;
         stopButton.disabled = true;
         recognition = null; // Nullify for clean restart next time
     };
     // --- End handleRecognitionStartError Helper ---


    // --- REWRITTEN stopListening Function (Enhanced Cleanup) ---
    const stopListening = () => {
        // If not actively listening and no pending AI processing/transcript, nothing to stop
        if (!isListening && !isProcessingAI && finalTranscript.trim().length === 0) {
            updateStatus('Ù„ÙŠØ³ Ù‡Ù†Ø§Ùƒ Ø´ÙŠØ¡ Ù„Ø¥ÙŠÙ‚Ø§ÙÙ‡.', 'info');
            return;
        }

        console.log("Attempting to stop listening. Current state: isListening=", isListening, "isProcessingAI=", isProcessingAI, "finalTranscript length=", finalTranscript.trim().length);

        isManuallyStopped = true; // Set this flag FIRST to indicate user intent
        isListening = false; // Update listening state immediately

        // Indicate stopping in UI, but only if not currently processing AI.
        // If processing AI, the status will switch to 'idle' after AI finishes.
        if (!isProcessingAI) {
            updateStatus('Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù...', 'info');
        }

        if (recognition) {
            console.log("Calling recognition.stop()...");
            recognition.stop(); // This will trigger the `onend` event.
            // The cleanup (recognition = null for manual stop) and final status update happens in `onend`.
            // Do NOT nullify recognition here; let `onend` handle it for proper lifecycle.
        } else if (finalTranscript.trim().length > 0 && !isProcessingAI) {
            // If recognition object is null (e.g., due to an error before starting)
            // but there's a pending transcript, process it.
            console.log("Recognition not active, but pending transcript exists. Processing last segment.");
            clearTimeout(speechTimeout);
            processFinalTranscriptSegment(finalTranscript.trim()).finally(() => {
                // After processing the last chunk, if not listening and not processing AI, set status to idle
                if (!isListening && !isProcessingAI) updateStatus('ØªÙ… Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù', 'idle');
            });
            finalTranscript = ''; // Clear transcript variable
            interimTranscriptElem.textContent = ''; // Clear display
            interimTranscript = ''; // Clear variable
            // Since `recognition` was already null, enable buttons here
            startButton.disabled = false;
            stopButton.disabled = true;
        } else {
            // If nothing to stop (no listening, no pending transcript, no AI processing)
            console.log("stopListening called but no active state found to explicitly stop.");
            clearTimeout(speechTimeout);
            // Status should already be idle or error if nothing was active/processing
            if (!isProcessingAI) updateStatus('ØªÙ… Ø§Ù„Ø¥ÙŠÙ‚Ø§Ù', 'idle'); // Fallback status update
            startButton.disabled = false;
            stopButton.disabled = true;
            // `recognition` should already be null or handled by a previous error state.
        }
        // Button states and final status update are now primarily handled by onend (for recognition.stop)
        // or the specific else blocks above.
    };
    // --- END REWRITTEN stopListening Function ---


    // --- MODIFIED displayDiff Function ---
    const displayDiff = () => {
        const suggested = suggestedReplyElem.value;
        const agent = agentReplyElem.value;
        // Adjusted placeholders to match current HTML and logic
        const placeholderSuggested = 'Ø³ÙŠØ¸Ù‡Ø± Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ø³ÙŠØ¸Ù‡Ø± Ù‡Ù†Ø§...';
        const placeholderAgent = 'Ø§ÙƒØªØ¨ Ø±Ø¯Ùƒ Ù‡Ù†Ø§ Ø£Ùˆ Ø§Ù†Ø³Ø® Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­...';

        // Determine the actual text for comparison, treating specific placeholders as empty
        const textSuggested = (suggested === placeholderSuggested || !suggested) ? '' : suggested;
        const textAgent = (agent === placeholderAgent || !agent) ? '' : agent;


        // If both are effectively empty, show no comparison message
        if (!textSuggested && !textAgent) {
             diffOutputElem.innerHTML = 'Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù‚Ø§Ø±Ù†Ø© Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†.';
             return;
        }

        // Check if Diff library is available
        if (typeof Diff === 'undefined') {
             diffOutputElem.innerHTML = 'Ø®Ø·Ø£: Ù…ÙƒØªØ¨Ø© Diff ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©.';
             return;
        }

        // Calculate the diff
        const diff = Diff.diffWords(textSuggested, textAgent);
        const fragment = document.createDocumentFragment();

        if (diff.length === 0 && textSuggested === textAgent) {
             // If identical and not empty, show the text
             fragment.appendChild(document.createTextNode(textSuggested || 'Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù…ØªØ·Ø§Ø¨Ù‚Ø©.'));
        }
        else if (diff.length === 0) {
             // This case should ideally not happen if inputs differ, but as a fallback:
             fragment.appendChild(document.createTextNode('Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù…Ø®ØªÙ„ÙØ© ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ† Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©.'));
        }
        else {
            // Build the diff HTML fragment
            diff.forEach(part => {
                // green for additions, red for deletions, default span for common parts
                const color = part.added ? 'ins' : part.removed ? 'del' : 'span';
                const node = document.createElement(color);
                node.appendChild(document.createTextNode(part.value));
                fragment.appendChild(node);
            });
        }

        diffOutputElem.innerHTML = ''; // Clear previous content
        diffOutputElem.appendChild(fragment); // Append the new diff HTML
    };
    // --- END MODIFIED displayDiff Function ---


    const populateCannedResponses = () => {
        // Clear existing options except the default placeholder if any
        cannedResponsesSelect.innerHTML = '<option value="">Ø§Ø®ØªØ± Ø±Ø¯Ø§Ù‹ Ø¬Ø§Ù‡Ø²Ø§Ù‹...</option>';
        for (const [key, value] of Object.entries(cannedResponses)) {
            const option = document.createElement('option');
            option.value = value;
            // Show a snippet of the canned response
            const displayValue = value.length > 50 ? value.substring(0, 50) + '...' : value;
            option.textContent = `${key}: ${displayValue}`;
            cannedResponsesSelect.appendChild(option);
        }
    };

    // --- localStorage Persistence Functions ---
    const saveToLocalStorage = (key, value) => {
        try {
            localStorage.setItem(key, value);
        } catch (e) {
            console.error('Error saving to localStorage:', e);
        }
    };

    const loadFromLocalStorage = (key, defaultValue) => {
        try {
            const value = localStorage.getItem(key);
            return value !== null ? value : defaultValue;
        } catch (e) {
            console.error('Error loading from localStorage:', e);
            return defaultValue;
        }
    };
    // --- End localStorage Persistence Functions ---


    // Event Listeners
    startButton.addEventListener('click', startListening);
    stopButton.addEventListener('click', stopListening);
    // --- MODIFIED copyReplyButton Listener ---
    copyReplyButton.addEventListener('click', () => {
        // Check against both possible placeholder texts and empty string
        if (suggestedReplyElem.value && suggestedReplyElem.value !== 'Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ù…Ù† Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯.' && suggestedReplyElem.value !== 'Ø³ÙŠØ¸Ù‡Ø± Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ø³ÙŠØ¸Ù‡Ø± Ù‡Ù†Ø§...') {
            agentReplyElem.value = suggestedReplyElem.value;
            displayDiff(); // Update diff immediately after copying
            saveToLocalStorage('agentReplyContent', agentReplyElem.value); // Save to localStorage
            updateStatus('ØªÙ… Ù†Ø³Ø® Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­ Ø¥Ù„Ù‰ Ù…Ø±Ø¨Ø¹ Ø§Ù„Ø±Ø¯', 'success');
            // Add !isProcessingAI check to timeout
            setTimeout(() => { if (!isListening && !isProcessingAI) updateStatus('Ø¬Ø§Ù‡Ø²', 'idle'); }, 2000);
        } else {
            updateStatus('Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø¯ Ù…Ù‚ØªØ±Ø­ Ù„Ù†Ø³Ø®Ù‡.', 'error');
            // Add !isProcessingAI check to timeout
            setTimeout(() => { if (!isListening && !isProcessingAI) updateStatus('Ø¬Ø§Ù‡Ø²', 'idle'); }, 2000);
        }
    });
    // --- END MODIFIED copyReplyButton Listener ---

    // --- Added localStorage save on input ---
    agentReplyElem.addEventListener('input', () => {
        displayDiff();
        saveToLocalStorage('agentReplyContent', agentReplyElem.value);
    });
    jobContextElem.addEventListener('input', () => {
        saveToLocalStorage('jobContextContent', jobContextElem.value);
    });
    // --- End localStorage save on input ---


    suggestedReplyElem.addEventListener('input', displayDiff); // Keep this to update diff if suggestedReply changes programmatically

    cannedResponsesSelect.addEventListener('change', (event) => {
        // When a canned response is selected, put it in the agent reply box
        if (event.target.value) {
            agentReplyElem.value = event.target.value;
            displayDiff(); // Update diff immediately
            saveToLocalStorage('agentReplyContent', agentReplyElem.value); // Save to localStorage
        }
    });
    useCannedButton.style.display = 'none'; // Hide this button as the select handles selection

    // --- Added Send Reply Button Listener ---
    if (sendReplyButton) { // Check if the button element exists
        sendReplyButton.addEventListener('click', () => {
            if (agentReplyElem.value.trim() !== '' && agentReplyElem.value.trim() !== 'Ø§ÙƒØªØ¨ Ø±Ø¯Ùƒ Ù‡Ù†Ø§ Ø£Ùˆ Ø§Ù†Ø³Ø® Ø§Ù„Ø±Ø¯ Ø§Ù„Ù…Ù‚ØªØ±Ø­...') { // Added placeholder check
                updateStatus('ØªÙ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø¯ (Ù…Ø­Ø§ÙƒØ§Ø©)', 'success');
                console.log("Agent's reply sent (simulated):", agentReplyElem.value);
                // Add !isProcessingAI check to timeout
                setTimeout(() => { if (!isListening && !isProcessingAI) updateStatus('Ø¬Ø§Ù‡Ø²', 'idle'); }, 2000);
            } else {
                updateStatus('Ø§Ù„Ø±Ø¯ ÙØ§Ø±Øº. Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø´ÙŠØ¡ Ù„Ø¥Ø±Ø³Ø§Ù„Ù‡.', 'error');
                 // Add !isProcessingAI check to timeout
                setTimeout(() => { if (!isListening && !isProcessingAI) updateStatus('Ø¬Ø§Ù‡Ø²', 'idle'); }, 2000);
            }
        });
    } else {
        console.warn("Send Reply Button element with id 'sendReplyButton' not found in HTML.");
    }
    // --- End Added Send Reply Button Listener ---


    // --- Initial setup ---
    // populateCannedResponses(); // Canned responses are not defined in the provided snippet.
    // If you have a `cannedResponses` object, uncomment this line.

    // Load saved content from localStorage on page load
    jobContextElem.value = loadFromLocalStorage('jobContextContent', '');
    agentReplyElem.value = loadFromLocalStorage('agentReplyContent', '');

    displayDiff(); // Set initial diff display based on loaded content
    updateStatus('Ø¬Ø§Ù‡Ø²', 'idle'); // Set initial status clearly

    // Initial state setup for buttons - these are now managed inside updateStatus,
    // but explicitly setting here ensures initial correct state on load.
    startButton.disabled = false;
    stopButton.disabled = true;

    // Check microphone permission state on load and update status message if needed
    if (navigator.permissions && typeof navigator.permissions.query === 'function') {
        navigator.permissions.query({ name: "microphone" }).then(permissionStatus => {
            console.log("Initial microphone permission state on load:", permissionStatus.state);
            if (permissionStatus.state === 'denied') {
                updateStatus('Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ù…Ø±ÙÙˆØ¶. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„ÙˆØµÙˆÙ„ ÙÙŠ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…ØªØµÙØ­.', 'error');
            } else if (permissionStatus.state === 'prompt') {
                 updateStatus('Ø¬Ø§Ù‡Ø² (ÙŠØªØ·Ù„Ø¨ Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡)', 'idle');
            }
             // If already granted, status is already 'Ø¬Ø§Ù‡Ø²' which is fine.
        }).catch(err => {
             console.warn("Failed to query microphone permission state on load:", err);
             // Fallback if query fails
             updateStatus('Ø¬Ø§Ù‡Ø² (ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡)', 'idle');
        });
    } else {
         console.warn("navigator.permissions API not available on load.");
         updateStatus('Ø¬Ø§Ù‡Ø² (ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø°Ù† Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† Ø¹Ù†Ø¯ Ø§Ù„Ø¨Ø¯Ø¡)', 'idle');
    }
    // --- End Initial setup ---

</script>
</body>
</html>
